{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af208d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Causal Inference Crash Course\n",
    "## Defining Some Causal Models\n",
    "Julian Hsu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee885fe4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview \n",
    "* This presentation will define some propensity-matching based  models:\n",
    "    1. Ordinary Least Squares (OLS)\n",
    "    2. Propensity Binning with Regression adjustment\n",
    "    3. Inverse propensity weighting \n",
    "    4. Double machine learning - Partial Linear Model (PLM)\n",
    "    5. Double machine learning - Interactive Regression Model (IRM)\n",
    "* For each model, we will define the estimator and its properties.\n",
    "    * So yes, there will be a lot of math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a2311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordinary Least Squares (OLS)\n",
    "* We have an outcome $Y_i$, pre-treatment features $X_i$, and a treatment indicator $W_i$. We want to know the causal relationship between $Y_i$ and $W_i$.\n",
    "* We can estimate this relationship by estimating an OLS model:\n",
    "$$ Y_i = \\beta X_i + \\tau W_i + \\epsilon_i $$\n",
    "* Where $(\\hat{\\beta}, \\hat{\\tau})$ are estimated to minimize the mean squared error:\n",
    "$$ argmin_{\\hat{\\beta}, \\hat{\\tau}} \\big\\{ \\frac{1}{N} \\sum^N_{i=1}(Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i )^2 \\big\\}$$\n",
    "* We know OLS is simple but why is it causal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6b028",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is OLS Causal?\n",
    "\n",
    "* The OLS model estimate $(\\hat{\\beta}, \\hat{\\tau})$ based on the assumption that the mean squared error is zero, conditional on $(X_i, W_i)$.\n",
    "* This is defined in the moment conditions: $E[ (Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i) \\times X_i]=0$ and  $E[ (Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i) \\times W_i]=0$.\n",
    "    * After conditioning on $X_i$, the unexplained variation in $Y_i$ is mean independent of treatment.\n",
    "     * Therefore, they corresponds to the unconfoundedness assumption\n",
    "* Under mean independence, $\\hat{\\tau}$ is an unbiased ATE/ATET estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69180b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS as a propensity-based matching model\n",
    "* OLS implicitly estimates a propensity score \n",
    "* Recall that the OLS estimator is:\n",
    "$$\\hat{\\beta} = (X'_i X_i)^{-1} X_i'Y_i = \\dfrac{cov(X_i, Y_i)}{var(X_i)}$$\n",
    "* From the Frisch-Waugh-Lovell theorem, we can be more specific about $\\hat{\\tau}$̂. (Appendix has more details)\n",
    "    $$ \\hat{\\tau} = \\dfrac{cov(\\tilde{W}_i, Y_i)}{var(\\tilde{W}_i)}$$\n",
    "* Where $\\tilde{W}_i = E[W_i | X_i]     - W_i$. Well, $E[W_i | X_i]$ is the propensity score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db6ca7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Helpful reference on the use of weighting in regressions [Weighting Regressions by Propensity Scores, Freedman and Berk 2008](https://www.stat.berkeley.edu/~freedman/weight.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d904aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propensity Binning with Regression Adjustment\n",
    "* What is Regression Adjustment?\n",
    "* Recall the potential outcome (Neyman-Rubin) framework:\n",
    "    * Average Treatment Effect on the Treated (ATET) $= Y_i(1,1) - Y_i(1,0)$\n",
    "    * Average Treatment Effect (ATE) $= E_1[Y_i(1,1), Y_i(0,1)] - E_0[Y_i(0,0), Y_i(1,0)]$\n",
    "        * Where $E_x$ is the weighted average of observed and counterfactuals for $W=x$\n",
    "*  The problem is that we do not observe $y_i(0,1)$ and $Y_i(1,0)$\n",
    "* Regression Adjustment model asks: \"What if we treat estimating $Y_i(0,1)$ and $Y(_i(1,0)$ as a pure prediction problem and predict out of sample?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a6b16",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Regression adjustment is just a \"T-learner\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1818580",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Adjustment Algorithm\n",
    "1. Start with the control $(W_i=0)$ sample. Train your favorite ML model to predict $Y_{i,W_i=0}$ using $X_{i,W_i=0}$. Call this trained model $g_0(X_i)$. Do the same with the treatment $(W_i=1)$ sample, call the trained model $g_1(X_i)$.\n",
    "2. Estimate the outcomes under treatment and control with $g_0(X_i)$ and $g_1(X_i)$. This gives you $\\hat{Y}_i(1,1),\\hat{Y}_i(0,1),\\hat{Y}_i(1,0)$ and $\\hat{Y}_i(0,0)$.\n",
    "3. Estimate ATE or ATET:\n",
    "    1. $\\hat{ATET} = \\hat{Y}_i(1,1) - \\hat{Y}_i(1,0)$\n",
    "    2. $\\hat{ATE} = E_1[\\hat{Y}_i(1,1), \\hat{Y}_i(0,1)] - E_0[\\hat{Y}_i(0,0), \\hat{Y}_i(1,0)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae017166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Adjustment is OLS\n",
    "* Regression Adjustment is OLS in disguise.\n",
    "* Intuitively, this is because the OLS is also extrapolating the potential outcomes with a single model, instead of multiple. (Appendix has the technical explanation.)\n",
    "* Therefore, Regression Adjustment implicitly estimates a propensity score because OLS does too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15588e52",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can also go full circle and say that propensity score matching is the same as regression adjustment: https://blog.stata.com/2016/08/16/exact-matching-on-discrete-covariates-is-the-same-as-regression-adjustment/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e5c6fb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os as os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1217a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propensity Binning as Insurance Against Outliers\n",
    "\n",
    "![Image](Figures/SomeCausalModels_Figure_1.png)\n",
    "* We show in the dotted lines the predicted outcomes for controland treatment.\n",
    "* But we have outliers , based on propensity score, which are influencing our predicted outcomes. \n",
    "* How can we flexibly accommodate the outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027a107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propensity Binning with Regression Adjustment\n",
    "* We estimate a propensity score, $\\hat{P}_i(X_i)$ and then divide the data into segments of $\\hat{P}_i(X_i)$.\n",
    "\n",
    "* For each segment, implement the Regression Adjustment model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ccc80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse Propensity Weighting \n",
    "* This approach is inspired by sampling methods.\n",
    "* Suppose you have :\n",
    "    * (A) treatment observation with a propensity score of 0.99\n",
    "    * (B) treatment observation with a propensity score of 0.01\n",
    "* You most likely have a lot of (A), but not a lot of (B). So you want to give (B) more weight in your analysis because it happens very rarely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dd0a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse Propensity Weighting Model Definition\n",
    "* The Inverse Propensity Weighting (IPW) estimator for the ATE is:\n",
    "$$ \\dfrac{1}{N} \\sum^N_{i=1} \\Big[ \\dfrac{W_i Y_i}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}(X_i)} \\Big]$$\n",
    "* For ATET:\n",
    "$$ \\dfrac{1}{N} \\sum^N_{i=1} \\hat{P}(X_i) \\Big[ \\dfrac{W_i Y_i}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}(X_i)} \\Big]$$\n",
    "* We will unpack the ATE to intuitively understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66437aa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse Propensity Weighting Intuition\n",
    "$$ \\dfrac{1}{N} \\sum^N_{i=1} \\Big[ \\dfrac{W_i Y_i}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}(X_i)} \\Big]$$\n",
    "\n",
    "* When we divide a treatment observation by the propensity score, we are increasing its importance when it is less likely to be treated.\n",
    "* Note that the denominator of $\\dfrac{1}{N} \\dfrac{1}{\\hat{P}(X_i)}$  approximates taking the average of just the treated observations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a9ed6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages and Disadvantages\n",
    "* **Advantage**: Weighting provides flexible form and only requires diagnosing with propensity score\n",
    "* **Disadvantage**: since we divide by the propensity score, you risk imprecise estimates if you have a lot of propensity scores near zero or one. \n",
    "    * This means the variance can explode and be very large.\n",
    "* Solutions are:\n",
    "    * Drop these observations;\n",
    "    * Replace these observations’ propensity scores with a pre-determined value (like 0.001 or 0.999) or unconditional probability of treatment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66bf1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Double Machine Learning (DML)\n",
    "* Yes, we are finally here.\n",
    "* At a high-level, DML is a more flexible version of the previous models\n",
    "* Note that DML relies on the same assumptions as the other models presented here\n",
    "* We will cover the propensity-matching based models from the Chernozhukov et al. (2016) paper.\n",
    "    * Partial Linear Model\n",
    "    * Interactive Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8049731",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can ML do for causal inference?\n",
    "* Off-the-shelf ML models also cannot be used for inference.\n",
    "    * Exceptions: generalized random forests (Athey et al. 2018); neural nets (Farrell et al. 2020)\n",
    "*  We can use ML in two ways:\n",
    "1. Estimate a better propensity score. Recall that for OLS: $\\hat{\\tau} = \\dfrac{cov(\\tilde{W}_i, Y_i)}{var(\\tilde{W}_i)}$\n",
    "2. Estimate better counterfactuals as we do for Regression Adjustment models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced57a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High-Level Strategy for DML\n",
    "* We use two ML strategies so we can do inference:\n",
    "1. Regularization based on residualization\n",
    "    * Based on Frisch-Waugh-Lovell theorem (recall this from the OLS slides?)\n",
    "    * Compare residuals that are constructed to be independent except due to the variation of interest (Neyman orthogonality)\n",
    "2. Sample-splitting to prevent overfitting\n",
    "    * Cross-validation is important to make sure prediction is not biased\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bcf2c",
   "metadata": {},
   "source": [
    "## DML – Partial Linear Model Motivation\n",
    "* The partial linear model is a form of OLS. We allow for a more flexible prediction of $Y_i$ and $W_i$:\n",
    "* OLS:\n",
    "\n",
    "\\begin{array}{rl}\n",
    "      Y_i & x = \\hat{\\beta} X_i + \\hat{\\tau} W_i + \\epsilon_i \\\\\n",
    "      Y_i & = g_0(X_i) + \\hat{\\tau} W_i + \\epsilon_i \\\\\n",
    "      Y_i - g_0(X_i) & = g_0(X_i) - g_0(X_i) + \\hat{\\tau} W_i + \\epsilon_i \\\\\n",
    "Y_i - g_0(X_i) & = \\hat{\\tau} W_i + \\epsilon_i \\\\      \n",
    "\\end{array} \n",
    "* We replace $\\hat{\\beta} X_i $ with $g_0(X_i)$.  From this we can show that estimating a regression of the residualized $Y_i$ based on $g_0(X_i)$  on $W_i$ estimates the treatment effect.\n",
    "• OLS also requires a residualized $W_i$, from the Frisch-Waugh-Lovell theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca15f2c",
   "metadata": {},
   "source": [
    "## DML – Partial Linear Model Setup\n",
    "* The partial linear model is a form of OLS.\n",
    "* OLS:\n",
    "$$       Y_i  = \\hat{\\beta} X_i + \\hat{\\tau} W_i + \\epsilon_i \n",
    "$$\n",
    "* DML - Partial Linear Model:\n",
    "$$       Y_i  = g_0(X_i) + \\hat{\\tau} W_i + \\epsilon_i $$\n",
    "$$ W_i = m_0(X_i) + \\nu_i$$\n",
    "* Assumptions are still the same as OLS (especially unconfoundedness)\n",
    "* We still assume the treatment effect is linearly additive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1be9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partial Linear Model Procedure\n",
    "$$       Y_i  = g_0(X_i) + \\hat{\\tau} W_i + \\epsilon_i $$\n",
    "$$ W_i = m_0(X_i) + \\nu_i$$\n",
    "\n",
    "* First Stage:\n",
    "    1. Predict $Y_i$ using $X_i$ with sample-splitting, get $\\hat{Y}_i$.\n",
    "    2. Predict $W_i$ using $X_i$ with sample-splitting, get $\\hat{W}_i$.    \n",
    "    3. Calculate the residuals for $Y_i$ and $W_i$. Specifically, $\\tilde{Y}_i = Y_i - \\hat{Y}_i$ and $\\tilde{W}_i = W_i - \\hat{W}_i$.\n",
    "\n",
    "* Second Stage:\n",
    "    * Estimate the OLS model:\n",
    "    $$ \\tilde{Y_i} = \\hat{\\tau}\\tilde{W}_i + \\zeta_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37590a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What ML models can be used?\n",
    "* You can use essentially any ML model for the first stage to generate $\\hat{Y}_i$ and $\\hat{W}_i$.\n",
    "* These are called the “nuisance parameters” because we care about the quality of the prediction, not the theoretical properties of the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae53bd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DML – Interactive Regression Model\n",
    "* The partial linear model is simple and intuitive because it is a more flexible version of OLS.\n",
    "* Despite this, we are restricted by the assumption that treatment linearly interacts with the outcome.\n",
    "* It also assumes that the treatment effect is the same for all observations. Specifically, that the average treatment effect (ATE) is the same as the average treatment effect on the treated (ATET).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ed0f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interactive Regression Model (IRM) Specification\n",
    "$$ \\hat{\\tau}_{ATE} = E\\Big[ (\\hat{Y}_{1,i} - \\hat{Y}_{0,i}) +  \\dfrac{W_i (\\hat{Y}_{i} - \\hat{Y}_{1,i})}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) (\\hat{Y}_{i} - \\hat{Y}_{0,i})}{1-\\hat{P}(X_i)} \\Big]$$\n",
    "\n",
    "$$ \\hat{\\tau}_{ATET} = E\\Big[ \\dfrac{W_i (\\hat{Y}_{i} - \\hat{Y}_{1,i})}{P \\times \\hat{P}(X_i)}  - \\dfrac{\\hat{P}(X_i)(1-W_i) (\\hat{Y}_{i} - \\hat{Y}_{0,i})}{P\\times 1-\\hat{P}(X_i)} \\Big]$$\n",
    "\n",
    "* When we estimate ATET, we no longer need $\\hat{Y}_{1,i}$\n",
    "* Estimating the IRM model has the same first stage as PLM. The only difference is the second stage.\n",
    "* Now we will explain the components of ATE, which generalize to ATET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74a9a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explaining the IRM Model, Part 1\n",
    "$$ \\hat{\\tau}_{ATE} = E\\Big[ (\\hat{Y}_{1,i} - \\hat{Y}_{0,i}) +  \\dfrac{W_i (\\hat{Y}_{i} - \\hat{Y}_{1,i})}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) (\\hat{Y}_{i} - \\hat{Y}_{0,i})}{1-\\hat{P}(X_i)} \\Big]$$\n",
    "\n",
    "* This first part is just regression adjustment, which is bias if there is large estimation error\n",
    "* The second parts are the estimation error of the outcome for treatment and control units, which are weighted by propensity scores\n",
    "* We combine regression adjustment and propensity weighting for a “doubly robust” approach (more details in the appendix) where we can correct for our regression adjustment estimates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c464c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explaining the IRM Model, Part 2\n",
    "* Re-writing the previous equation:\n",
    "\n",
    "\\begin{array}{rl}\n",
    " \\hat{\\tau}_{ATE} & = E\\Big[ (\\hat{Y}_{1,i} - \\hat{Y}_{0,i}) +  \\dfrac{W_i (\\hat{Y}_{i} - \\hat{Y}_{1,i})}{\\hat{P}(X_i)}  - \\dfrac{(1-W_i) (\\hat{Y}_{i} - \\hat{Y}_{0,i})}{1-\\hat{P}(X_i)} \\Big] \\\\\n",
    "  & = E\\Big[ (\\hat{Y}_{1,i} +\\dfrac{W_i (\\hat{Y}_{i} - \\hat{Y}_{1,i})}{\\hat{P}(X_i)} )  -  - (\\hat{Y}_{0,i} - \\dfrac{(1-W_i) (\\hat{Y}_{i} - \\hat{Y}_{0,i})}{1-\\hat{P}(X_i)} ) \\Big]\n",
    " \\end{array}\n",
    "* Therefore, we are correcting our estimates of $\\hat{Y}_{1,i}$ and $\\hat{Y}_{0,i}$\n",
    "* Applying this observation level correction means that there is variation at treatment estimates at the observation level.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b3731",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "* This presentation defines some propensity-matching based models:\n",
    "    1. Ordinary Least Squares (OLS) \n",
    "    2. Propensity Binning with Regression adjustment\n",
    "    3. Inverse propensity weighting \n",
    "    4. Double machine learning - Partial Linear Model (PLM)\n",
    "    5. Double machine learning - Interactive Regression Model (IRM)\n",
    "* Remember they all rely on the same assumptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04e836",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ac6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frisch-Waugh-Lovell Theorem\n",
    "* The OLS estimator is based on the Frisch-Waugh-Lovell theorem, or “partialing out”.\n",
    "* Pay attention. This is the same trick behind double machine learning.\n",
    "•* Start with:\n",
    "$$cov(\\tilde{W}_i, Y_i) = cov(\\tilde{W}_i, \\beta X_i + \\tau W_i + \\epsilon_i)$$\n",
    "    * Know that $cov(\\tilde{W}_i, \\beta X_i) = 0$ because $\\tilde{W}_i = E[W_i | X_i] - W_i$, so it already conditions on $X_i$.\n",
    "    * Also know that $cov(\\tilde{W}_i, \\epsilon_i) =0$ for the same reason.\n",
    "    \n",
    "* Then $cov(\\tilde{W}_i, Y_i) = cov(\\tilde{W}_i, \\tau W_i) = \\tau var(\\tilde{W}_i) $    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddcf282",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is LASSO an improvement on OLS?\n",
    "\n",
    "\n",
    "| model | Ordinary Least Squares | Least Absolute Shrinkage and Selection Operator |\n",
    "|--- |:---|---|\n",
    "| Objective Function | $argmin_{\\hat{\\beta}, \\hat{\\tau}} \\{ \\frac{1}{N}\\sum^N_{i=1} (Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i)^2 \\}$ | $argmin_{\\hat{\\beta}, \\hat{\\tau}} \\{ \\frac{1}{N}\\sum^N_{i=1} (Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i)^2 \\}$, subject to $\\sum^J_{j=1} |\\hat{\\beta}_j| + |\\hat{\\tau}| \\leq C$ |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* LASSO regression coefficients are chosen to maximize prediction, subject to a constraint in the parameters. \n",
    "* Intuitively, it assumes that coefficients are zero and there are penalties non-zero coefficients.\n",
    "* Certainly, LASSO has better out-of-sample prediction. But can we use it for causal inference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00e14f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we use LASSO for causal inference?\n",
    "* No, we can’t. Here is a technical and intuitive explanation.\n",
    "* Technically, OLS identifies the causal estimate because of this moment condition you can get from solving the optimization problem:\n",
    "$$E[ (Y_i - \\hat{\\beta}X_i - \\hat{\\tau}W_i) \\times X_i]=0$$\n",
    "But you can’t get this from a LASSO.\n",
    "* Intuitively, a LASSO coefficient has two interpretations: the causal estimate of $\\hat{\\tau}$ and a feature selection of whether $W_i$ is important to the prediction problem.\n",
    "    * Then the unconfoundedness assumption may no longer hold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884252a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix Slides – Doubly Robust Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162688d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doubly Robust\n",
    "* What if the propensity score is wrong? \n",
    "* It can be wrong because we do not have enough features, or we have the incorrect model specification.\n",
    "* This causes a problem because recall that we can estimate the treatment \n",
    "effect using the difference between treatment status and the propensity \n",
    "score. Recall from the OLS slides:\n",
    " $$\\hat{\\tau} = \\dfrac{cov(\\tilde{W}_i, Y_i)}{var(\\tilde{W}_i)}$$\n",
    " where $\\tilde{W}_i = E[W_i | X_i] - W_i$.\n",
    "* Ideally $\\tilde{W}_i$ represents the conditionally random variation in treatment. But if the propensity model is wrong, then it also incorporates model error. \n",
    "* Then, we have the wrong value for $\\hat{\\tau}$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e524974",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model mis-specification\n",
    "* The same logic applies to predicting the counterfactual outcome.\n",
    "* Model mis-specification isn’t completely solved with a good ML model. \n",
    "* For example, a counterfactual outcome prediction is an extrapolation exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1c04f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doubly Robust Mental Model\n",
    "* A “doubly robust” model attempts to address model mis-specification. \n",
    "* It incorporates:\n",
    "     1. propensity score; and \n",
    "     2. counterfactual models \n",
    "    such that it will give you the correct value for $\\hat{\\tau}$, as long as one of the models is correct.\n",
    "* There are a lot of functional forms, but we’ll show a popular one next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efcda5",
   "metadata": {},
   "source": [
    "## Augmented Inverse Propensity Weight (AIPW) Model Definition\n",
    "* from Robins, Rotnitzky, and Zhao (1994)\n",
    "* ATE:\n",
    "\n",
    "$$ \\dfrac{1}{N}\\sum^N_{i=1} \\big[ \\dfrac{W_i Y_i}{\\hat{P}} - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}} \\big] - \\dfrac{W_i - \\hat{P}}{\\hat{P}(1-\\hat{P})}[(1-\\hat{P})](\\hat{Y}_{i,1}) + \\hat{P}(\\hat{Y}_{i,0}) $$\n",
    "* We will build intuition why this works under misspecification of either \n",
    "$\\hat{P}$ or $\\hat{Y}_{i,1} / \\hat{Y}_{i,0}$.\n",
    "* This is similar to a double machine learning implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851f289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AIPW - $\\hat{Y}_{i,1} / \\hat{Y}_{i,0}$ are wrong\n",
    "$$ \\dfrac{1}{N}\\sum^N_{i=1} \\big[ \\dfrac{W_i Y_i}{\\hat{P}} - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}} \\big] - \\dfrac{W_i - \\hat{P}}{\\hat{P}(1-\\hat{P})}[(1-\\hat{P})](\\hat{Y}_{i,1}) + \\hat{P}(\\hat{Y}_{i,0}) $$\n",
    "* If $\\hat{Y}_{i,1} / \\hat{Y}_{i,0}$ are wrong, but $\\hat{P}$ is right, then then $\\hat{P} = E[W_i|X_i]$ so term  $W_i - \\hat{P} $ is zero in expectation.\n",
    "* Therefore, in expectation, we are left with this:\n",
    "$$ \\dfrac{1}{N}\\sum^N_{i=1} \\big[ \\dfrac{W_i Y_i}{\\hat{P}} - \\dfrac{(1-W_i) Y_i}{1-\\hat{P}} \\big]$$\n",
    "* Which is the inverse propensity weighting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23a505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AIPW - $\\hat{P}$ is wrong\n",
    "* Note we can re-arrange the AIPW estimator to look like this:\n",
    "$$ \\dfrac{1}{N}\\sum^N_{i=1} \\big[ \\dfrac{W_i(Y_i -\\hat{Y}_{i,1} }{\\hat{P}} + \\hat{Y}_{i,1} - \\dfrac{(1-W_i)(Y_i - \\hat{Y}_{i,0}}{W_i - \\hat{P}} - \\hat{Y}_{i,0} \\big] $$\n",
    "\n",
    "* If $\\hat{P}$ is wrong, but $\\hat{Y}_{i,1} / \\hat{Y}_{i,0}$ are right, then the difference in observed and predicted outcomes are zero in expectation.\n",
    "* Therefore, in expectation, we are left with this:\n",
    "$$ \\dfrac{1}{N}\\sum^N_{i=1} \\big[\\hat{Y}_{i,1} - \\hat{Y}_{i,0}\\big]$$\n",
    "* Which is the regression adjustment model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b29e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Misspecification Problems\n",
    "* Doubly robust methods allow us to have misspecification in $\\hat{P}$or $\\hat{Y}_{i,1} / \\hat{Y}_{i,0}$. \n",
    "* We can get even more flexible with ML models and relax functional forms.\n",
    "* Flexibility becomes important if $X_i$ becomes high dimensional.\n",
    "* Note that $X_i$ include generated features. \n",
    "    * For example:\n",
    "    * Interact statuses with previous spending\n",
    "    * Basis function transformations of previous spending (polynomials, segments, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71182393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix Slides –Instrumental Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8f142",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrumental Variables (IV) - Motivation\n",
    "* The unconfoundedness assumption may not be satisfied. There is selection bias into $W_i$ unexplained by $X_i$\n",
    "* Are we blocked? Not necessarily.\n",
    "* Suppose we have a feature $Z_i$ that directly determines $W_i$ but does not directly determine $Y_i$\n",
    "* In other words, $Z_i$ only affects $Y_i$ through $W_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad349d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IV Two-Stage Model \n",
    "$$ Y_i = \\beta X_i + \\tau W_i + \\epsilon_i$$\n",
    "$$ W_i = \\alpha X_i + \\gamma Z_i + \\psi_i $$\n",
    "* We can use this model to estimate $\\tau$ in a two stage process.\n",
    "1. Estimate $W_i$, get $\\hat{W}_i$\n",
    "2. Estimate $Y_i = \\hat{\\beta} X_i + \\hat{\\tau} W_i + \\epsilon_i$\n",
    "* Why does this work? We study the necessary assumptions and intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef129ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IV Assumptions\n",
    "$$ Y_i = \\beta X_i + \\tau W_i + \\epsilon_i$$\n",
    "$$ W_i = \\alpha X_i + \\gamma Z_i + \\psi_i $$\n",
    "\n",
    "1. Exclusion Restriction: $cov(\\epsilon, Z_i) = 0$, in other words $Z_i$ is uncorrelated to $Y_i$ conditional on $X_i$ and $W_i$/\n",
    "     * Example of a violation: suppose that we want to know the impact of a missed delivery date ($W_i$) on future spending ($Y_i$). We propose using an extreme weather event ($Z_i$), like an earthquake, as an instrument for getting a missed promise. This would not work because an earthquake would affect future spending for reasons unrelated to a missed delivery occurring.\n",
    "2. Strong instrument: $cov(W_i, Z_i) \\neq 0$, in other words $Z_i$ is a strong predictor of $W_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a228312",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuitively, why does this work?\n",
    "* In the second stage, we estimate $Y_i = \\hat{\\beta} X_i + \\hat{\\tau} W_i + \\epsilon_i$\n",
    "* Then, variation in $\\hat{W}_i$ depends on $X_i$ and $Z_i$. Let’s focus on the variation due to $Z_i$.\n",
    "* If we think back to the Frisch-Waugh-Lovell theorem, the OLS coefficient is based on the difference between $W_i$ and $\\hat{W}(X_i)$. Here in the IV setting, we are looking at the difference between $\\hat{W}(X_i, Z_i)$ and \n",
    "$\\hat{W}(X_i)$.\n",
    "* Therefore, we are relying in variation in $W_i$ based on $Z_i$ which we assume is exogeneous to$Y_i$ conditional on $X_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a648df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
