{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391a1837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Inference Crash Course\n",
    "# Heterogeneous Treatment Effect Models and Inference\n",
    "Julian Hsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc2da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Inference Series\n",
    "1. Foundations\n",
    "2. Defining some Causal Models\n",
    "3. Inference, Asymptotic Theory, and Bootstrapping\n",
    "4. Best Practices: Outliers, Feature Selection, and Bad Control\n",
    "5. Heterogeneous Treatment Effect Models \n",
    "6. Arguable Validation\n",
    "7. Panel Data\n",
    "8. Regression Discontinuity Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41702a09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "* This presentation covers the general problem of estimating \n",
    "heterogeneous treatment effects (HTE) and how it differs from \n",
    "ATE/ATET estimation.\n",
    "* Covers a few models:\n",
    "    * Linear models via Double Machine Learning\n",
    "    * Causal Forests / Local Linear Forests\n",
    "    * Doubly Robust models following Kennedy (2020)\n",
    "* Wrap up with a simulation demonstration\n",
    "* Mentions about extensions to panel models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88d989",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* T-Learner and X-Learner as building blocks for Doubly Robust\n",
    "* S-Learner is just DML\n",
    "**Reference**:\n",
    "* T-learner is regression adjustment model, with a counterfactual outcome model trained on the control and treatment samples\n",
    "* S-learner is controlling for everything in a the single regression\n",
    "* X-learner is weighting regression adjustment output by propensity score.\n",
    "* econml's [Overview of Formal Methodology](https://econml.azurewebsites.net/spec/estimation/forest.html#overview-of-formal-methodology) and [metalearners](https://econml.azurewebsites.net/spec/estimation/metalearners.html#x-learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bec2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE Overview\n",
    "* \"What's the effect of faster shipping?\" Average treatment effect (ATE) and average treatment effect on the treated (ATET) models want to know aggregate treatment effects.\n",
    "* \"What's the effect of faster shipping on customer in rural areas? / Which customers benefit the most from faster shipping?\" HTE model want to estimate the distribution of treatment effects or for a specific subset or individuals\n",
    "\n",
    "$$Y_i = \\hat{\\beta} X_i + \\hat{\\tau}(Z_i) W_i + \\epsilon_i $$\n",
    "* $ \\hat{\\tau}(Z_i)$ is the HTE and it varies of $Z_i$. We keep $X_i$ different from $Z_i$ for more flexible notation.\n",
    "* We can also call this the conditional average tratment effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e27a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE as an estimated function\n",
    "* We want to estimate the functional form of HTE.\n",
    "* When estimating ATE/ATET, we are only concerned with the average. \n",
    "     * We average over more granular treatment effects. \n",
    "*  Estimating more granular treatment effects means there are \n",
    "additional challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded2a28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How much variation do we want in our HTE function?\n",
    "*  Two extremes:\n",
    "1. Individualized treatment estimates allow more flexibility, but can \n",
    "demand large sample sizes and variation in data.\n",
    "* Increases the risk of noise driving estimates\n",
    "2. Segmented estimates are the least flexible, with the least risk of \n",
    "noise driving estimates. \n",
    "* They can also be more interpretable for other applciations.\n",
    "• In-between case is to allow treatment effects to vary across some \n",
    "dimensions, but not others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a76a60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE ideal experiment\n",
    "* We can understand these two extreme based on what the ideal \n",
    "experiment is to estimate unbiased HTE.\n",
    "* For individualized HTE, the ideal is to randomize treatment for each \n",
    "individual. (impossible)\n",
    "* For segmented HTE, the ideal is to randomize treatment for each \n",
    "segment. (stratified randomization)\n",
    "* The more individualized HTE is, the more data and assumptions are \n",
    "needed to distinguish between real patterns and statistical noise in \n",
    "the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032429e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE inference challenge\n",
    "* Statistical inference for ATE/ATET estimates is based on the \n",
    "distribution of error around the average estimate. \n",
    "* The challenge is getting a distribution around an individual estimate.\n",
    "* The solution is to rely on either model specifications or \n",
    "bootstrapping-esque methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64a9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some HTE Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77410fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support across use cases\n",
    "\n",
    "|  | Cross Sectional Data | Panel Data | Continuous Treatment\n",
    "|---|---|---|---|\n",
    "|DML | Y | Y | Y|\n",
    "|Generalized Random Forests |Y | N | N|\n",
    "|Doubly Robust | Y | N | N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225701a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DML-Style models\n",
    "* Semenova, Goldman, Chernozhukov, Taddy (2021) - SGCT\n",
    "* Let's start with linearity assumptions, which allows interpretability:\n",
    "$$Y_i = \\hat{\\beta} X_i + \\hat{\\tau}(Z_i) W_i + \\epsilon_i $$\n",
    "* SGCT decomposes $\\hat{\\tau}(Z_i)$ into a functionl form:\n",
    "$$ \\hat{\\tau}(Z_i) \\rightarrow \\hat{\\tau} \\cdot g(Z_i) $$\n",
    "* where $g(Z_i)$ is different functions of $Z_i$. For example:\n",
    "$$ \\hat{\\tau} \\cdot g(Z_i) = \\hat{\\tau}_0 + \\hat{\\tau}_1 z_{1i} + \\hat{\\tau}_2 z_{1i}^2 $$\n",
    "* Continuing this example, the model would be: \n",
    "$$Y_i = \\hat{\\beta} X_i +  \\hat{\\tau}_0 W_i + \\hat{\\tau}_1 z_{1i} W_i + \\hat{\\tau}_2 z_{1i}^2 W_i + \\epsilon_i $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3c377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGCT users residualization\n",
    "* Now how do we estimate this equation?\n",
    "$$Y_i = \\hat{\\beta} X_i +  \\hat{\\tau}_0 + \\hat{\\tau}_1 z_{1i} + \\hat{\\tau}_2 z_{1i}^2 + \\epsilon_i $$\n",
    "* At first glance we can just do OLS, but we can improve that approach \n",
    "with double machine learning (DML; aka residualization).\n",
    "    * Recall DML works through the Frisch-Waugh-Lovell theorem\n",
    "* SGCT estimates this equation\n",
    "\n",
    "$$ \\tilde{Y}_i = \\hat{\\tau}_0  \\tilde{W}_i + \\hat{\\tau}_1 z_{1i} \\tilde{W}_i + \\hat{\\tau}_2 z_{1i}^2 \\tilde{W}_i + \\epsilon_i $$\n",
    "* Where $\\tilde{Y}_i$ and $\\tilde{W}_i$  are the residualized outcome and treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51c0ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGCT – HTE and inference\n",
    "* We now need to do inference for individual treatment effects from\n",
    "\n",
    "$$ \\tilde{Y}_i = \\hat{\\tau}_0  \\tilde{W}_i + \\hat{\\tau}_1 z_{1i} \\tilde{W}_i + \\hat{\\tau}_2 z_{1i}^2 \\tilde{W}_i + \\epsilon_i $$\n",
    "\n",
    "* HTE is $\\hat{\\tau}_1 z_{1i} + \\hat{\\tau}_2 z_{1i}^2 $, where the standard error is calculated via the Delta method.\n",
    "\n",
    "* We can use OLS to estimate the above equation if:\n",
    "    * • There are few dimensions of heterogeneity (ie $g(Z_i)$ is low dimensional); or\n",
    "    * We are interested in specific dimensions of heterogeneity (ie we only want to know HTE across user tenure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9b079",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGCT – inference with post-LASSO regression\n",
    "\n",
    "* A problem is if we estimate with all possible transformations of $z_{1i}$. In other words, overfitting.\n",
    "* We can select the relevant transformations of $z_{1i}$ with LASSO regression, but then we cannot do inference.\n",
    "* Get around this with a sample-splitted LASSO for inference. Select features with LASSO on one half of the dataset, and then estimate HTE using those selected features on the other half.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698c4aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Random Forests \n",
    "*  Causal forests are a special class of generalized random forests, which we will discuss here.\n",
    "* As motiviation, note that under the unconfoundedness assumption, we assume that treatment is random:\n",
    "$$E [ (Y - \\hat{g}(X_i) - \\hat{\\tau}(Z_i)W_i)W_i] = 0 $$\n",
    "* In other words, $\\hat{\\tau}(Z_i)$ satisfy the orthogonality assumption similar to Frisch-Waugh-Lovell. \n",
    "* The problem is that we do not know what $\\hat{\\tau}(Z_i) $ looks like, so we want a flexible specification. Ideally, something non-parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c016e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRF – Causal Forest Objective Function\n",
    "* The estimating equation (with simplified notation is):\n",
    "$$ \\big( \\hat{\\tau}(Z_i), \\hat{g}(X_i) \\big) = argmin \\big\\{ E[ \\alpha_i(z) (Y_i - \\hat{g}(X_i) - \\hat{\\tau}(Z_i) W_i | Z_i = z]^2  ) \\big\\}$$\n",
    "* What's the purpose of $\\alpha_i(z)$?\n",
    "* $\\alpha_i(z)$ is a weight used to allow flexibility in $\\hat{\\tau}(Z_i)$.\n",
    "    * Can be estimated to kernel methods but performance suffers under high dimensions\n",
    "    * Estimate $\\alpha_i(z)$  with a random forest to deal with high dimensionality of $Z_i$!\n",
    "* This weight gives us enough variation in $\\hat{\\tau}(Z_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdb47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRF - Weights $\\alpha_i(z)$\n",
    "* $\\alpha_i(z)$  represents the probability that a training sample $i$ falls into the same leaf as sample $z$, across different trees in a random forest\n",
    "* Splits in the random forest used to estimate $\\alpha_i(z)$ are determined to \n",
    "maximize variation $\\hat{\\tau}(Z_i)$ across splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c887fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRF - Inference\n",
    "* Athey, Tibshirani, and Wager (2019) show that $\\hat{\\tau}(Z_i)$ is asymptotically normal.\n",
    "* This is because  $\\alpha_i(z)$ is estimated in an “honest” (Athey and Wager, \n",
    "2018) fashion, where different samples are used to determine splits in $\\alpha_i(z)$ and $\\hat{\\tau}(Z_i)$.\n",
    "* Standard errors and confidence intervals are available based on a bootstrap/jackknife approach.\n",
    "* Intuitively, estimate the distribution in $\\hat{\\tau}(Z_i)$  when $z$ is removed from the sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52ae85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doubly Robust - Kennedy (2020)\n",
    "* Recall the interactive regression model from DML:\n",
    "$$ \\hat{\\tau}_{ATE} = E \\big[ (\\hat{Y}_{1,i} - \\hat{Y}_{0,i} ) + \\dfrac{W_i (Y_i - \\hat{Y}_{1,i})}{\\hat{P}_i} -  \\dfrac{(1-W_i) (Y_i - \\hat{Y}_{0,i})}{1-\\hat{P}_i} \\big]$$\n",
    "* Recall that we can intuitively understand this as a individual-level comparison from a regression adjustment model, correcting for prediction errors.\n",
    "* Removing the expectation, we can see that these are individual-level treatment effect estimates\n",
    "$$  (\\hat{Y}_{1,i} - \\hat{Y}_{0,i} ) + \\dfrac{W_i (Y_i - \\hat{Y}_{1,i})}{\\hat{P}_i} -  \\dfrac{(1-W_i) (Y_i - \\hat{Y}_{0,i})}{1-\\hat{P}_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319129a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. This doubly robust formula is really flexible and you can use for panel models as well, such as in Sant'Anna and Zhou.\n",
    "2. Think of this as a form of an X-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fd84c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying inference to the individual estimates\n",
    "* The problems are that these estimates:\n",
    "1. Are meant to be averaged to get the ATE/ATET; and \n",
    "2. Do not have inference properties.\n",
    "* Kennedy (2020) frames these as “noisy” estimates of the true HTE, and proposes “refining” them with a second stage.\n",
    "$$ hte_i =  (\\hat{Y}_{1,i} - \\hat{Y}_{0,i} ) + \\dfrac{W_i (Y_i - \\hat{Y}_{1,i})}{\\hat{P}_i} -  \\dfrac{(1-W_i) (Y_i - \\hat{Y}_{0,i})}{1-\\hat{P}_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0a14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ed281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f17ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294c728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "* Recall, HTE models are not about estimating the average effect, but  rather the functional form of treatment effects.\n",
    "* The additional complexity of functional form can make this very difficult.\n",
    "* We will demonstrating using simulation evidence, where we can change the **true HTE function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107b033d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os as os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2b62b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First Example - Linearity\n",
    "\n",
    "* HTE $= 2x$\n",
    "* Model controls: $x, x^2 $\n",
    "\n",
    "![Image](Figures/HTE_Figure_1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cbcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
