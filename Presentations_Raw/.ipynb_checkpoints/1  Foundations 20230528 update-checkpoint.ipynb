{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd0643f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Inference Crash Course:\n",
    "# Foundations\n",
    "Julian Hsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95559b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview \n",
    "* This is intended to be the first in a series of causal inference\n",
    "* This presentation will cover:\n",
    "    * The fundamental problem of causal inference\n",
    "    * Necessary assumptions for valid causal inference\n",
    "    * Working example with simulated data\n",
    "* The intended audience is a scientist familiar with basic statistics and unfamiliar with causal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c2d22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamental problem of causal inference\n",
    "* Suppose we want to estimate the return of investment of going to \n",
    "college on career earnings.\n",
    "* Data:\n",
    "     * Earnings of college-goers: $Y_i (W_i=1)$\n",
    "    * Earnings of non-college-goers: $Y_i (W_i=0)$\n",
    "* Can we just compare the earnings of college-goers and non-college\u0002goers?\n",
    "    * Naïve difference: $ Y_i (W_i=1) - Y_i (W_i=0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea27ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do we know what happens in the counterfactual situation?\n",
    "* We cannot, because college-goers tend to be different from non-college-goers. \n",
    "* In other words, we do not think that the observed earnings of non-college-goers accurately represents the earnings of college-goers if they did not go to college, and vice versa.\n",
    "    * In other words, correlation is not causation.\n",
    "* We formalize this with the “potential outcomes” mental model (aka \n",
    "Neyman-Rubin causal model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ae134",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potential outcomes framework\n",
    "* There is a difference between what we observe, and what we wish we could observe.\n",
    "\n",
    "| | If they did not go to college | If they went to college |\n",
    "|---|---|---\n",
    "|Non-College goers | $ Y_i (0,0)$, observed | $ Y_i (0,1)$, counterfactual |\n",
    "|College goers | $ Y_i (1,0)$, counterfactual | $ Y_i (0,1)$, observed |\n",
    "\n",
    "\n",
    "* We only observe $Y_i (0,0)$ and $Y_i (1,1)$ but we want to observe the counterfactuals.\n",
    "* • If we think that $Y_i (1,1) - Y_i (0,0)$ is the real effect of going to college, \n",
    "then we are also assuming that $Y_i (1,1) = Y_i (0,1)$ and  $Y_i (0,0) = Y_i (1,0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefe101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection Bias\n",
    "\n",
    "Why would  $Y_i (1,1) \\neq Y_i (0,1)$? If college goers did so because they \n",
    "believed they would get more out of it than those who did not go.\n",
    "    * College attendees may be higher ability and thus get more out of college\n",
    "* This is called selection bias\n",
    "* Causal inference models can address selection bias – under certain \n",
    "conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428dd425",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal inference and reporting\n",
    "* The usual metric for reporting purposes is the Average Treatment Effect on the Treated (ATET). \n",
    "    * ATET answers “What is the impact of going to college for college-goers?”\n",
    "    * aka: $Y_i (1,1) = Y_i (1,0)$\n",
    "* Causal inference cares about the Average Treatment Effect (ATE) and ATET.\n",
    "     * ATE answers “What is the impact of going to college?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe9001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three necessary assumptions for valid causal inference\n",
    "\n",
    "1. Unconfoundedness assumption$^1$: treatment status is random once we condition based on pre-treatment features $X_i$\n",
    "    * Conditioning based on $X_i$ is equivalent to comparing synthetic twins\n",
    "    * AKA – there is no omitted variable bias\n",
    "* Setup for an Example: \n",
    "    * Observed outcome: $Y_i = f(X_1, X_2, \\psi) + \\tau W(X_1, X_2, \\epsilon) $\n",
    "    * $f(X_1, X_2, \\psi)$ is a function that determines outcome for control accounts\n",
    "    * $\\tau$ is the treatment effect\n",
    "    * $W(X_1, X_2, \\epsilon)$ is a function that determines whether observation is in treatment or control\n",
    "    \n",
    "$^1$ aka selection-on-observables, conditional independence, ignorability, conditional independence assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdd3d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scenario 1: the ideal\n",
    "* Observed outcome: $Y_i = f(X_1, X_2, \\psi) + \\tau W(X_1, X_2, \\epsilon) $\n",
    "* $X_1, X_2$ are observed\n",
    "* $\\psi, \\epsilon$ are not observed, but are independent of each other\n",
    "* Therefore, variation in treatment and the outcome conditional on \n",
    "$X_1, X_2 $ are independent of each other.\n",
    "    * aka cov$(Y | X_1, X_2,  , W|X_1,X_2)=0$\n",
    "* Then conditioning on $X_1, X_2$ yields an unbiased estomate if $\\tau$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c50e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scenario 2: not ideal\n",
    "* Observed outcome: $Y_i = f(X_1, X_2, \\psi) + \\tau W(X_1, X_2, \\epsilon) $\n",
    "* $X_1, X_2$ are observed\n",
    "* $\\psi, \\epsilon$ are not observed, but *are correlated*\n",
    "* Therefore, variation in treatment and the outcome conditional on \n",
    "$X_1, X_2 $ are *not* independent of each other.\n",
    "    * aka cov$(Y | X_1, X_2,  , W|X_1,X_2) \\neq 0$\n",
    "* Then conditioning on $X_1, X_2$ yields a biased estimate if $\\tau$\n",
    "    * Bias depends on how correlated $\\psi, \\epsilon$ are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92333a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what happens in the not ideal scenario?\n",
    "* You will still get an estimate for $\\tau$, but it will be biased.\n",
    "* Recall this  $\\psi, \\epsilon$ are not observed, but are correlated with each other.\n",
    "* You cannot identify what portion for $\\hat{\\tau}$ is due to the true treatment effect $\\tau$, and what is due to $\\psi$ or $\\epsilon$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4efcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we know if we are in the ideal or not ideal scenario?\n",
    "* You don’t know.\n",
    "* Whether the unconfounded assumption is met depends on whether the unobserved  $\\psi, \\epsilon$ are correlated. But  $\\psi, \\epsilon$ are unob*erved by definition*\n",
    "* So what do we do?\n",
    "* We rely on our knowledge of the causal inference problem so that we can safely assume that  $\\psi, \\epsilon$ are not correlated.\n",
    "* We can also gather suggestive evidence that unconfoundedness is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc44eca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Valid causal inference relies on having the “perfect” but “imperfect” prediction problem\n",
    "\n",
    "* Observed outcome: $Y_i = f(X_1, X_2, \\psi) + \\tau W(X_1, X_2, \\epsilon) $\n",
    "* Imagine if you observed $X_1, X_2, \\epsilon$ and $\\psi$ , so you could perfectly predict $Y$ and $W$ . Can you estimate $\\tau$?\n",
    "* No – because you cannot figure out how much of $Y$  is due to $f()$ and \n",
    "due to $ \\tau W()$\n",
    "* In other words, there is no variation in treatment and the outcome \n",
    "conditional on $X_1,X_2$. So there is no variation to compare to estimate \n",
    "$\\tau$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42db671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomized control trials / Experiments / AB tests\n",
    "* When treatment is unconditionally random, then the unconfoundedness assumption is automatically satisfied.\n",
    "* This means a t-test is an unbiased estimate, but controlling for features $X$ results in a more precise estimate.\n",
    "    * This is because a t-test ignores variation correlated with $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92b9bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three necessary assumptions for valid causal inference - SUTVA\n",
    "2. Stable unit treatment value assumption: control units do not influence treatment units.\n",
    "* A violation of this would be college-goers help non-college-goers get higher paying jobs\n",
    "* We can only test this if we observe how treatment and control observations are connected. (Network data)\n",
    "* In practice, this is usually assumed away because of how difficult it is to get network data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751fad1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating treatment effects using matching\n",
    "* We can use the unconfoundedness assumption for causal inference. \n",
    "* Intuitively: Let’s find treatment accounts and control accounts that \n",
    "have the same $X$ values. Using the unconfoundedness assumption, \n",
    "conditional on having same $X$ values, treatment is random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c65f6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact and KNN Matching Examples\n",
    "* For example, suppose we only have one feature $ X \\sim U[1,4]$\n",
    "* Exact Matching: We would find treatment and control accounts that have the same value of $X$. You assume that accounts with the same value of $X$ are randomly assigned treatment status.\n",
    "* KNN Matching: Instead of having the exact value of X, we could match treatment and control accounts that have similar values of X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdd07b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching leads to the curse of dimensionality\n",
    "* But what if we have a lot of pre-treatment features $X$? \n",
    "* For example, suppose we have ten pre-treatment features, each taking a value of 1,2,3,or 4. Then we have 410 = 1,048,576 possible combinations. \n",
    "* We can’t practically find exact, or approximate, matches across a lot of $X$. This is a dimensionality problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63210cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propensity Score Matching\n",
    "* Rosenbaum and Rubin (1983)$^1$ find that instead of matching on each value for each $X$, we can match on one feature instead. \n",
    "* Match on the predicted probability of treatment, $\\hat{p}(X_i)$ known as the propensity score.\n",
    "    * Propensity score model can be trained from an ML model, as long as you have the treatment label information\n",
    "* Intuitively, conditional on the propensity score, treatment status is random. This uses the unconfoundedness assumption. \n",
    "* For example, you find a treatment unit with  $\\hat{p}(X_i) = 0.50$ and a control unit with  $\\hat{p}(X_i) = 0.50$ . For these units, whether they are  in treatment or control is random, given that they have the same $\\hat{p}(X_i) = 0.50$\n",
    "\n",
    "$^1$ Rosenbaum, P. and Rubin, D. 1983. “The central role of the propensity score in observational studies for causal effects.” \n",
    "Biometrika. Vol 70-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035d03f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three necessary assumptions for valid causal inference - Overlap\n",
    "3. Overlap assumption: we can find control and treatment accounts with similar propensity score $\\hat{p}(X_i)$.\n",
    "- This one we can verify if we have the true propensity score. We can plot the distribution of propensity scores for treatment and control groups, and see if there is reasonable overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9378d482",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os as os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6abcb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ideal Scenario\n",
    "\n",
    "\n",
    "![Image](Figures/Foundation_Figure1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301a4eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not Ideal Scenario \n",
    "\n",
    "![Image](Figures/Foundation_Figure2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c19138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3bd8c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Propensity-based models are the most common models\n",
    "* Many causal models are versions of propensity score matching:\n",
    "    * Ordinary least squares (OLS)\n",
    "    * Propensity Binning with Regression adjustment\n",
    "    * Inverse propensity weighting\n",
    "    * Double machine learning\n",
    "* They all rely on the same assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b67488",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example with Simulated Data\n",
    "* Jupyter Notebook link [here](https://github.com/shoepaladin/causalinference_crashcourse/blob/main/Notebooks/1%20Foundations.ipynb)\n",
    "\n",
    "* To make sure we can tell whether the average treatment effect estimate is correct, we will use fake data. \n",
    "* We will setup our fake data, and show the bias under different causal models and whether the assumptions for causal inference are met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803d451",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup with fake data\n",
    "* We have four iid features, $x_1, x_2, x_3,$ and $x_4$ from normal distributions.\n",
    "* The observed outcome is:\n",
    "* $Y = f(x_1,x_2,x_3,x_4,\\psi) + \\tau W(x_1,x_2,x_3,x_4, \\epsilon)$\n",
    "    * Where $(\\psi, \\epsilon)$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e701d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We study three scenarios\n",
    "* (Ideal scenario) we observed $x_1, x_2, x_3,x_4$\n",
    "* (Unconfoundedness assumption violated) We only observe $x_1, x_2, x_3$ and do not observe $x_4$\n",
    "* (Overlap assumption violated) We observe $x_1, x_2, x_3,x_4$, but control units with certain $\\hat{p}($x_1, x_2, x_3,x_4$)$ values are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb00616",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We study four models\n",
    "* We estimate ATE with four different causal models:\n",
    "    1. OLS\n",
    "    2. Propensity Binning with Regression adjustment \n",
    "    3. Double machine learning - Partial Linear Model\n",
    "    4. Double machine learning - Interactive Regression Model\n",
    "* Let’s skip the model definitions for now. Theoretically they should give the same results; but estimates will vary over context.\n",
    "* In each model, we condition on $x_1, x_2, x_3,x_4$ and $x_1^2, x_2^2, x_3^2,x_4^2$ flexibility. We could condition on even more transformations of \n",
    "for additional flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7701394",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias in ATE Estimate across five models and three scenarios\n",
    "* Showing the average bias $= (\\tau - \\hat{\\tau})$ across 500 simulated datasets. For reference, $\\tau=5$ in each simulation.\n",
    "\n",
    "| | Ideal Scenario | Unconfoundedness Violated | Overlap Violated|\n",
    "|---|---|---|---|\n",
    "|OLS | 0.003 | 0.003 | 0.722|\n",
    "|Propensity Binning with Regression Adjustment | -0.62 | 0.115 | -1.722 |\n",
    "|DML - Partial Linear | 0.040 | 0.248 | 0.909 |\n",
    "|DML - Interactive Regression | 0.036 | 0.292 | 2.186 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ae96d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "* This presentation covered some fundamentals for causal inference:\n",
    "    * The fundamental causal inference problem\n",
    "    * Assumptions for unbiased estimates\n",
    "    * Working example\n",
    "*  Please let me know what else interests you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8288536",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250aab7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details on Modeling Approaches\n",
    "* The DML-IRM and IPW approaches exclude observations with estimated propensity scores below 0.001 and 0.999 to avoid dividing by a small number, which would inflate estimates.\n",
    "* Except for OLS, all nuisance functions to predict the outcome and treatment status are estimated using four-fold cross-validated LASSO and Logistic regressions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
