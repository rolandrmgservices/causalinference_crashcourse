{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47675a81",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Outline**\n",
    "0. Be clear that we are going to study cross-sectional models and rely on unconfoundedness and exogeneity assumptions.\n",
    "    1. Fits A/B experiments and where we can first apply DML\n",
    "    2. Before we get into complexities of HTE, we should always first look at ATE so we can diagnose while things are simple.\n",
    "1. HTE overview\n",
    "    1. Understanding fine-grained (individual, segmented) treatment effects. Useful when we want to do personalized targeting.\n",
    "    2. We can think of HTE as an estimated function, rather than a scalar.\n",
    "\n",
    "\n",
    "* T/R/S/X Learner sources:\n",
    "    * [econml meta-learner documentation](https://econml.azurewebsites.net/spec/estimation/metalearners.html)\n",
    "    * [hte_tutorial](https://gsbdbi.github.io/ml_tutorial/hte_tutorial/hte_tutorial.html)\n",
    "\n",
    "\n",
    "\n",
    "1. **model: T2X Learners**\n",
    "    1. Nests T-learner and X-learner methods\n",
    "    2. Really flexible but suspectible to noise\n",
    "        1. *Start to see a theme that imposing structure and assumptions can help a lot*\n",
    "        2. *Simulation example needed* Additional bias/noise because there is extrapolation into space without support.\n",
    "    3. Refinement through GML and \"filtering\" methods\n",
    "2. **model:  OLS**\n",
    "    1. Nests R-learner\n",
    "3. **model:  ML Weights: GRF, Neural Nets, and others**\n",
    "    1. \n",
    "    \n",
    "    \n",
    "4. Things not covered\n",
    "    1. Panel models\n",
    "5. Fun mentions\n",
    "    1. Integration with surrogate models\n",
    "    2. Additional structural assumptions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a1837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Inference Crash Course\n",
    "# Heterogeneous Treatment Effect Models and Inference\n",
    "Julian Hsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b867b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "* Use cases for heterogeneous treatment effects (HTE) models.    \n",
    "* Additional challenges compared to non-HTE models\n",
    "* Showcase a few families of HTE models:\n",
    "    1. Flexible learners and \"filtering\" \n",
    "    2. OLS and DML\n",
    "    3. ML-driven weights: GRF, Neural Nets, and others\n",
    "    * An on-going theme will be the benefits of making structural or functional form assumptions\n",
    "    * We will only cover cross-sectional models here for simplicity\n",
    "* Conclude with fun extensions and a lot of citations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d781c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When do we care about hetergeneous treatment effects (HTE)?\n",
    "* We can use data to recommend a universal policy:\n",
    "    1. What's the federal minimum wage?\n",
    "    2. Product return policy\n",
    "    3. Product pricing\n",
    "    *  These are not good use cases for heterogeneous treatment effects (HTE)\n",
    "* We also want to make targeted policies or take customized actions:\n",
    "    1. Which customers should be defaulted to faster delivery options?\n",
    "    2. How do we match sellers with the best support or representatives?\n",
    "    3. Which users see which sort of ads?\n",
    "    4. Which orders should we scrutize and delay for fraud investigation? \n",
    "    * These are use cases for HTE.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34717f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE use cases\n",
    "* Just like non-HTE use cases, like Average Treatment Effect (ATE) or Average Treatment Effect on the Treated (ATET), HTE remains a *causal question.*\n",
    "* Like all causal questions, the foundational problem is that we not observe the outcomes under both the treatment and control conditions.\n",
    "* This means it inherits all the causal complexities from its ATE/ATET cousins, and more.\n",
    "    * Cross-sectional:  overlap, unconfoundedness, etc.\n",
    "    * Panel: parallel/overlaping trends\n",
    "    * Your ears perk up when you can do a randomized experiment\n",
    "* You should **always** first estimate ATE/ATETs before trying HTE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027fe40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE modeling and notation\n",
    "* Suppose you have a dataset with:\n",
    "    * $Y_i$, outcome\n",
    "    * $X_i$, features or confounders\n",
    "    * $Z_i$, features you want to explore heterogeneity in\n",
    "    * $W_i$, treatment indicator\n",
    "\n",
    "* When we think about ATE/ATET, we can estimate by taking the difference between the outcomes under treatment and control: \n",
    "$$ \\tau = E[Y_1 (X_i) - Y_0 (X_i) ]$$\n",
    "    * Recall that the causal problem is that we only observe $Y_1$ for treated ($W_i=1$) units or $Y_0$ for control ($W_i=0$) units, not both.\n",
    "    * Therefore I am controlling for $X_i$. Again, this doesn't guarantee you have the correct estimate, but is common practice when assumptions hold.\n",
    "* For HTE, we are interested in variation across $Z_i$:\n",
    "$$ \\tau^{hte} (Z_i) = E[Y_1(X_i) - Y_0(X_i)  | Z_i]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfca8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I will adopt an OLS type equation here so it's easier to make the point that we are estimating a function instead of a scalar.\n",
    "\n",
    "* If we were to adop and OLS perspective, which is useful for the OLS/DML approach, ATE/ATET would be estimated with:\n",
    "\n",
    "$$ Y_i = \\hat{\\beta}X_i + \\hat{\\tau} W_i + \\epsilon_i$$\n",
    "* HTE has us instead estimate:\n",
    "\n",
    "$$ Y_i = \\hat{\\beta}X_i + \\hat{\\tau}(Z_i) W_i + \\epsilon_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958819c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTE interpretations\n",
    "$$ \\tau^{hte} (Z_i) = E[Y_1 - Y_0  | Z_i]$$ \n",
    "\n",
    "* We can also call this the conditional average treatment effect (CATE)\n",
    "* Since our estimate is no longer a single scalar number, but a function that inputs $Z_i$, we are now estimating an HTE function.\n",
    "* Your use case can allow different interpretations. \n",
    "    * \"How being offered a five-day versus two-day delivery options impacts different customers.\"\n",
    "    * \"How the average seller reacts to being treated with different services.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec10c7b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os as os\n",
    "\n",
    "## Figure of arXiv papers\n",
    "arXiv_paper_hits = pd.DataFrame(data={'year':[2017,2018,2019,2020,2021,2022],\n",
    "                                     'hte_hits':[8,13,15,26,38,66],\n",
    "                                     'causal_inf_hits':[78,118,154,219,305,446 ]})\n",
    "\n",
    "hte_yoy_growth = [ (arXiv_paper_hits['hte_hits'].iloc[a] - arXiv_paper_hits['hte_hits'].iloc[a-1])/arXiv_paper_hits['hte_hits'].iloc[a-1] for a in range(1,len(arXiv_paper_hits))]\n",
    "ci_yoy_growth = [ (arXiv_paper_hits['causal_inf_hits'].iloc[a] - arXiv_paper_hits['causal_inf_hits'].iloc[a-1])/arXiv_paper_hits['causal_inf_hits'].iloc[a-1] for a in range(1,len(arXiv_paper_hits))]\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(ncols=1,nrows=1, figsize=(5,3))\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Proportion Growth')\n",
    "\n",
    "ax.set_xticks(arXiv_paper_hits['year'])\n",
    "ax.set_xticklabels(labels=['{0:4.0f}'.format(a) for a in arXiv_paper_hits['year'].unique().tolist()])\n",
    "\n",
    "ax.plot(arXiv_paper_hits['year'][1:],hte_yoy_growth, marker='o', label='\"Heterogeneous\\n Treatment\\n Effect\"', color='seagreen')\n",
    "ax.plot(arXiv_paper_hits['year'][1:],ci_yoy_growth, marker='o', label='\"Causal\\n Inference\"', color='purple', linestyle='--')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1.0))\n",
    "ax.grid()\n",
    "ax.set_title('YoY Growth in Submitted arXiv Papers')\n",
    "fig.set_facecolor('lightsteelblue')\n",
    "plt.savefig(os.getcwd() + '/Figures/'+'HTE_Figure_0.png'\n",
    "           , bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "## Figure of model trained on treated sample\n",
    "datax = np.random.uniform(0,1,100)\n",
    "treatment = ( (np.exp(datax) / (1+np.exp(datax)) + np.random.uniform(-0.05,0.025, 100)) > 0.6).astype(float)\n",
    "datay = 0.25 + 0.5*np.log(1+datax) + 0.25*treatment + np.random.uniform(-0.05,0.05,100)\n",
    "df = pd.DataFrame(data={'W':treatment,\n",
    "                        'x':datax,\n",
    "                       'y':datay})\n",
    "df.sort_values(by='x',inplace=True)\n",
    "fig,ax = plt.subplots(ncols=1,nrows=1, figsize=(4,2))\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "\n",
    "ax.scatter(df.loc[df['W']==1]['x'],\n",
    "        df.loc[df['W']==1]['y'],  color='seagreen', label='Treated')\n",
    "ax.scatter(df.loc[df['W']==0]['x'],\n",
    "        df.loc[df['W']==0]['y'],  color='coral', label='Control')\n",
    "ax.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "ax.grid()\n",
    "ax.set_title('Predicting out of Sample')\n",
    "plt.savefig(os.getcwd() + '/Figures/'+'HTE_Figure_10a.png'\n",
    "           , bbox_inches='tight')\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611d822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some HTE Models\n",
    "This is a very active literative, so consider this just a brief summary of broad classes of HTE models\n",
    "<img src=\"Figures/HTE_Figure_0.png\" >\n",
    "\n",
    "\n",
    "|Class | categorical treatment | continuous treatment | cross-sectional data | panel data |\n",
    "|---:|:---|:---|:---|:---|\n",
    "|T+X (T2X) Learner | Y | N | Y | Sometimes |\n",
    "|OLS | Y | Y | Y | Y |\n",
    "|ML-Weights | Y | N  | Y | Sometimes |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdda415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common ingredients across models\n",
    "\n",
    "* Estimated counterfactual outcome, $\\hat{Y}_1(X_i), \\hat{Y}_0(X_i)$\n",
    "    * How would treated units perform if they were control units instead, and vice versa?\n",
    "* Estimated outcome, $\\hat{Y}(X_i)$\n",
    "* Propensity score, $\\hat{P}(X_i)$\n",
    "    * Probability that a given unit is treated.\n",
    "    * We will rely on this with the unconfoundedness/exogeneity assumption common in causal models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f7788",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## T+X (T2X) Learners - the big idea\n",
    "$$ \\tau^{hte} (Z_i) = E[Y_1(X_i) - Y_0(X_i)  | Z_i]$$ \n",
    "* Let's treat it as a prediction problem (\"T-Learner\").\n",
    "    * For each observation, predict $Y_1(X_i)$ and $Y_0(X_i)$ values    \n",
    "    * Use your favorite ML model to train two models 1: $Y_1(X_i)$; 2: $Y_0(X_i)$.\n",
    "* Calculate observation-level differences: $\\tau^{hte}_i = \\hat{Y}_1(X_i) - \\hat{Y}_0(X_i) $\n",
    "* We look at variation in $\\tau^{hte}_i$ across $Z_i$.    \n",
    "    * We can train a third prediction model of $\\tau^{hte}_i$ as a function of $Z_i$ for interpretability and reduce noise in $\\tau^{hte}_i$. We will come back to this in a few slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127956b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## T+X (T2X)  Learners - one question\n",
    "\n",
    "1. In the figure below, how are we sure that our $\\hat{Y}_1(X_i)$ model would do a good job predicting the outcome of the control units? \n",
    "    * We are extrapolating into an unknown space. We are training a model on treated units to predict the outcome of control units, so we do not have a ground truth to validate our $\\hat{Y}_1(X_i)$ model.\n",
    "\n",
    "<img src=\"Figures/HTE_Figure_10a.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b5d2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## T+X (T2X) Learners - relying on propensity scores\n",
    "* We rely on additional information, the propensity score $\\hat{P}(X_i)$\n",
    "* Relying on the unconfoundedness assumption, we can compare treated and control units with similar propensity scores to have the correct estimate.\n",
    "* A common way of doing this would to take a weighted average of the estimate for control and treated units (\"X-Learner\").\n",
    "* Künzel et al. (2017) propose taking the weighted average after predicting variation in $\\hat{\\tau}^{hte}_i$ across $Z_i$. We will revisit this in the next slide.\n",
    "\n",
    "$$ \\hat{\\tau}^{hte,x} = \\hat{P}(X_i) \\hat{\\tau}^0_i  + (1-\\hat{P}(X_i)) \\hat{\\tau}^1_i $$\n",
    "\n",
    "* Where $\\hat{\\tau}^0_i$ ($\\hat{\\tau}^1_i$) is the impact for control (treated) units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16088202",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## T+X (T2X) Learners - a second question\n",
    "\n",
    "2. Independent of the above question, how do we know that variation across $Z_i$ is real or due to noise?\n",
    "    * This is a big question we should ask of *every HTE model* and is an HTE-specific complexity. \n",
    "    \n",
    "*  Chernozhukov et al. (2017) propose using the propensity score and variation over $Z_i$ in a single equation, where you estimate how much variation in $\\hat{\\tau}^{hte}_i$ is driven by selected $Z_i$ while simultaneously controlling for the propensity score.\n",
    "* Kennedy (2020) propose estimating $\\hat{\\tau}^{hte}_i$ using $\\hat{Y}_1(X_i)$, $\\hat{Y}_0(X_i)$, and $\\hat{P}(X_i)$ in a single equation, and then estimating variation across $Z_i$. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c3358",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Summary - T2X Learners\n",
    "* In summary, we start with estimating the counterfactual outcomes (\"T-Learner\"). However, this is suspectiable to biases in predicting out of sample.\n",
    "* an \"X-Learner\" solves this by incorporating the propensity score so that we can estimate HTE with similar propensity scores. This relies on the unconfoundedness assumption.\n",
    "    * We should also look at variation across $Z_i$\n",
    "* There are a lot of approaches and sequences for doing this.  \n",
    "\n",
    "| Künzel et al. (2017) | Chernozhukov et al. (2017) |  Kennedy (2020)|\n",
    "|:--- |:---|:---|\n",
    "|1. Estimate  $\\hat{\\tau}^{hte}_i$ with a T-Learner. <br> 2. Predict $\\hat{\\tau}^{hte}_i$ over $Z_i$  <br> 3. Take weighted average with $\\hat{P}(X_i)$ <br> | 1. Estimate variation over $Z_i$ by debiasing with $\\hat{P}(X_i)$  | 1. Estimate $\\hat{\\tau}^{hte}_i$ with a doubly robust estimator combining counterfactual outcomes and $\\hat{P}(X_i)$. <br> 2. Predict variation over $Z_i$  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0040f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS\n",
    "* Where T2X Learner HTE models allow a lot of flexibility, we see that leveraging additional information, specifically the propensity score yields improvements.\n",
    "* We can simultaneously leverage both predictions of the outcome and propensity score with a common causal model, ordinary least squares (OLS)\n",
    "* We will improve on its framework with a double-debiased machine learning (DML) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2e33d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS - simple model\n",
    "* Recall that under the same assumptions as before, we can estimate the ATE/ATET with an OLS model:\n",
    "$$ Y_i = \\beta X_i + \\hat{\\tau} W_i + \\epsilon_i $$\n",
    "* We can incorporate HTE by including additional features\n",
    "$$ Y_i = \\beta X_i + \\hat{\\tau} W_i + \\hat{\\tau}^{hte,z} W_i \\times Z_i + \\epsilon_i $$\n",
    "* Therefore, the HTE is a baseline treatment is a combination fo the baseline treatment $\\hat{\\tau}$ with $\\hat{\\tau}^{hte,z} Z_i$.\n",
    "$$ \\hat{\\tau}^{hte} = \\hat{\\tau} + \\hat{\\tau}^{hte,z} Z_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a58877",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS - drawbacks of the simple model\n",
    "\n",
    "$$ Y_i = \\beta X_i + \\hat{\\tau} W_i + \\hat{\\tau}^{hte,z} W_i \\times Z_i + \\epsilon_i $$\n",
    "* This approach yields unbiased estimates for $\\hat{\\tau}^{hte}$. Interpretation is also very straight forward. However, we can face difficulties when:\n",
    "    1. The functions that determine $Y_i$ or $W_i$ cannot be well modeled linearly\n",
    "    2. $Z_i$ has high dimensionality; or\n",
    "    \n",
    "* We will solve both by incorporating approaches from double/debiased machine learning (DML) from Chernozhukov (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2f5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS - DML applied to HTE, Part 1\n",
    "* Semenova et al (2017) approach borrows the residualizing approach from DML, where we predict the observed outcome $\\hat{Y}(X_i)$ and propensity score $\\hat{P}(X_i)$.\n",
    "    * This is the \"first stage\" in DML. We then run the \"second stage\":\n",
    "$$ \\tilde{Y}_i = \\hat{\\tau}  \\tilde{W}_i + \\hat{\\tau}^{hte,z}  \\tilde{W}_i \\times Z_i + \\eta_i $$\n",
    "    * Where the $\\tilde{Y}_i$ and $\\tilde{W}_i$ are the difference between the predicted and observed outcome and treatment status, respectively.\n",
    "      \n",
    "* If we run this \"second stage\" as is, then we can still have problems if $Z_i$ is high-dimensional. We can think of this as a feature selection problem.\n",
    "    * We can have high dimensionality over different transformations of a variable. For example: $Z_i = [x_{1i}, x^2_{1i}, x^3_{1i}, log(x_{1i}), 1\\{x_{1i} > 0 \\} )]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7b6a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS - DML applied to HTE, Part 2\n",
    "* Semenova et al (2017) incorporates selection over $Z_i$ by adapting a sample-splitted LASSO regression.\n",
    "    * In general, LASSO regression coefficients do not have a causal interpretation. \n",
    "    * We get around this by doing sample-splitting\n",
    "$$ \\tilde{Y}_i = \\hat{\\tau}  \\tilde{W}_i + \\hat{\\tau}^{hte,z}  \\tilde{W}_i \\times Z_i + \\eta_i $$\n",
    "\n",
    "* We will split the sample into training and test samples. We select $Z_i$ on the training set using LASSO, and then estimate OLS on the selected $Z_i$ on the test set.\n",
    "    * We then average \"selected\" OLS coefficients across test samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b3545",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Notes - OLS and DML\n",
    "* We can get HTE through OLS as well, where we interact the treatment indicator with $Z_i$.\n",
    "* This works great under linearity assumptions and when $Z_i$ is low dimensional.\n",
    "* Otherwise, we can leverage concepts from DML. We residualize the outcome and treatment features to allow non-linearity in the components, and use a sample-splitted LASSO regression to select elements of $Z_i$.\n",
    "    * Is it worth noting that we are envoking elements of the Partial Linear Model from DML, where we still assume linearity in the second stage.\n",
    "    * The Interactive Regression Model from DML can also be applied to HTE, and is more similar to the T2X-Learners from the earlier set of slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd925927",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Weights\n",
    "* When we estimating differences between treatment and control units, we want to compare treatment and control units that are otherwise similar.\n",
    "    * If we compare the outcomes of treatment/control that are similar on observable characteristics and we assume exogeneity, then the difference is causal.     \n",
    "* We can do matching based on propensity scores. For example below. However, we may run into situations where a control unit can be a good match for multiple treated units (ie rows 1 and 3) \n",
    "\n",
    "| | Matched <br> Set | Treatment <br>Unit | Control <br> Unit | \n",
    "|---:|---:|:---:| :---:|\n",
    "| 1.| 1 | 0.20 | 0.21 |\n",
    "| 2.| 1 |  | 0.19 |\n",
    "| 3.| 2 | 0.23 | 0.22 |\n",
    "| 4.| 2 |  | 0.24 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda0646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weighting Units \n",
    "\n",
    "| | Matched <br> Set | Treatment <br>Unit | Control <br> Unit | \n",
    "|---:|---:|:---:| :---:|\n",
    "| 1.| 1 | 0.20 | 0.21 |\n",
    "| 2.| 1 |  | 0.19 |\n",
    "| 3.| 2 | 0.23 | 0.22 |\n",
    "| 4.| 2 |  | 0.24 |\n",
    "\n",
    "* We can weight control units based on how close they are in propensity scores or other distributional assumptions. \n",
    "* The more similar control and treated units are, the larger the weight should be.\n",
    "* Athey et al. (2018) use a Generalized Random Forest to determine weights that:\n",
    "    1. Compares similar treatment and control units; and\n",
    "    2. Explores variation across $Z_i$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f8dec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Tree is a building block for Generalized Random Forest (GRF)\n",
    "* A Generalized Random Forest (GRF) is a forest of Causal Tree (CT), rather than standard Decision Trees (DF).\n",
    "* We'll distinguish how standard Random Forests and Causal Forests would estimate $\\tau^{hte}$.\n",
    "* Both split the data sample based on having similar features $X_i$ to predict $\\tau^{hte}$. Splits are evaluated based on the variation in $\\tau^{hte}$ (ie entropy). This splitting process has two components. For a given potential split of a node:\n",
    "    1. How good is this bifurcation compared to other potential splits?\n",
    "    2. What's the predicted $\\tau^{hte}$ in each of the splits?\n",
    "* DT uses the same data to answer 1. and 2. In contrast, CT uses different data for each. Think of this as having one training sample determines splits, and the other training sample estimates $\\tau^{hte}$.\n",
    "    * You can think of 1. as a way to maximize prediction power, and 2. as an evaluation (for causal inference, estimating the effect).\n",
    "    * This allows us to calculate confidence intervals over CT and GRF's estimates. Note this is the same high-level approach \n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19879b93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRF approach\n",
    "* GRF calculates weights so that:\n",
    "    1. Control and treatment units with similar $X_i$ are compared to estimate $\\hat{\\tau}^{hte}$; and\n",
    "    2. It maximizes variation in $\\hat{\\tau}^{hte}$.\n",
    "* Note that this forces variation in $\\hat{\\tau}^{hte}$ across $X_i$, not necessarily $Z_i$. This is because the weighting is used to estimate the treatment effect based on matching and estimate heterogeneity. \n",
    "* We can improve upon this by taking the residualization concept from DML. That is, you first calculate $\\tilde{Y}_i$ and $\\tilde{W}_i$ and then train GRF to do variation across $Z_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae34394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac9acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9acbf0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Papers\n",
    "### T2X Learners:\n",
    "* Künzel, Sekhon, Bickel, Yu. *Meta-learners for estimating heterogeneous treatment effects using machine learning*  http://arxiv.org/abs/1706.03461\n",
    "* Semenova, Chernozhukov. *Debiased Machine Learning of Conditional Average Treatment Effects and Other Causal Functions* https://arxiv.org/abs/1702.06240\n",
    "* Chernozhukov, Demirer, Duflo, Fernández-Val. *Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments* https://arxiv.org/abs/1712.04802\n",
    "* Kennedy. *Towards optimal doubly robust estimation of heterogeneous causal effects* https://arxiv.org/abs/2004.14497\n",
    "* Sant'Anna, Zhao *Doubly Robust Difference-in-Differences Estimators* https://arxiv.org/abs/1812.01723\n",
    "\n",
    "### OLS:\n",
    "* Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins. *Double/Debiased Machine Learning for Treatment and Causal Parameters*  https://arxiv.org/abs/1608.00060\n",
    "* Semenova, Goldman, Chernozhukov, Taddy. *Estimation and Inference on Heterogeneous Treatment Effects in High-Dimensional Dynamic Panels under Weak Dependence* https://arxiv.org/abs/1712.09988\n",
    "\n",
    "### ML-Weights\n",
    "* Athey, Tibshirani, Wager. *Generalized Random Forests* https://arxiv.org/abs/1610.01271 \n",
    "* Friedberg, Tibshirani, Athey, Wager. *Local Linear Forests* https://arxiv.org/abs/1807.11408\n",
    "* Wager, Athey. *Estimation and Inference of Heterogeneous Treatment Effects using Random Forests* https://arxiv.org/abs/1510.04342\n",
    "* Farrell, Liang, Misra. *Deep Neural Networks for Estimation and Inference* https://arxiv.org/abs/1809.09953\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f62e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
