{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc4e621",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Causal Inference Crash Course \n",
    "## Inference\n",
    "Julian Hsu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476689b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview \n",
    "* This presentation will describe the “inference” in causal inference.\n",
    "1. Inference and consistency for OLS\n",
    "2. Challenge of applying asymptotic theory \n",
    "3. Bootstrapping is not a slow silver bullet\n",
    "* We will only focus on inference for the ATE/ATET and not HTE. HTE incorporates additional inference challenges we will cover as part of HTE models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac22fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical inference overview\n",
    "* Suppose we have a sample ($X$) and want to know whether its average is different from a given number, say zero. \n",
    "$$X = (x_1, x_2, ..., x_N) \\text{ and } X \\sim F(\\theta)$$\n",
    "\n",
    "* We want to know whether a new sample from $F(\\theta)$ would be different from zero on average.\n",
    "* Our null hypothesis is that the average of $X$ is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dc364",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis testing and confidence intervals\n",
    "* If we standardize the distribution of $X$, then we get a metric $t$ that we know is distributed by a Student’s t-distribution, which asymptotically approaches a normal distribution as the sample size increases\n",
    "$$ t = \\dfrac{\\bar{X} - 0}{se} $$\n",
    "where $se = \\frac{\\text{sample standard deviation}}{\\sqrt{N}}$,\n",
    "and $t \\rightarrow^d N(0,1) $.\n",
    "* This derivation relies on the Law of Large Numbers to that we can assume normality.\n",
    "* This statistic tests our null hypothesis that $\\bar{X} = 0$\n",
    "* This is useful because now we can model the variation in $X$ if we drew more samples.\n",
    "* We can now use this to form a confidence interval. A 95% confidence interval contains the range for 95% of future draws of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb01778",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS statistical inference\n",
    "* We can apply similar theories to do inference for an OLS regression\n",
    "$$Y+i = \\hat{\\beta} X_i + \\epsilon_i$$\n",
    "* We previously showed that $\\hat{\\beta}$ will be unbiased. But how do we know the estimates are not driven by noise?\n",
    "* Specifically, if we made another dataset, would we get the same value \n",
    "for $\\hat{\\beta}$?\n",
    "* In other words, what is the distribution of $\\hat{\\beta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ff272",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distribution of the OLS estimator\n",
    "* We will use that $\\hat{\\beta}$ is consistent and converges to the true value $\\beta$:\n",
    "\\begin{array}{rl}\n",
    "\\hat{\\beta} =& (X'X)^{-1}(X'y) \\\\\n",
    "=& (X'X)^{-1}(X'(X\\beta + \\epsilon)) \\\\\n",
    "=& (X'X)^{-1}(X'X\\beta) + (X'X)^{-1}X'\\epsilon \\\\\n",
    "=& \\beta + (X'X)^{-1}(X'\\epsilon)\n",
    "\\end{array}\n",
    "* How is $(X'X)^{-1}(X'\\epsilon)$ distributed? We can then show that:\n",
    "$$ \\sqrt{N}(\\hat{\\beta} - \\beta) \\rightarrow^d N(0, \\Sigma) $$\n",
    "* Where $\\Sigma = \\frac{1}{N}(X'X)^{-1}\\frac{1}{N}(X'\\epsilon \\epsilon'X) \\frac{1}{N}(X'X)^{-1}$\n",
    "* If we gathered more data and recalculated $\\hat{\\beta}$ the distribution of those calculations would asymptotically converge to $\\Sigma$\n",
    "* This now tells us the joint distribution of  $\\hat{\\beta}$. Now we can calculate confidence intervals. \n",
    "* See the Appendix for how to test hypothesis based on transformations of  $\\hat{\\beta}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6b5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference is not bias\n",
    "* Confidence intervals are about whether we would get the same estimates a certain proportion of the time.\n",
    "* A 95% confidence interval contains 95% of the possible estimates we would get from resampling the data. \n",
    "* But $\\hat{\\beta}$ could be biased. $\\hat{\\beta}$ can consistently estimate a biased value.\n",
    "$$ \\sqrt{N}(\\hat{\\beta} - \\beta) \\rightarrow^d N(\\text{bias}, \\Sigma) $$\n",
    "* Therefore, $\\hat{\\beta}$can be statistically significant and biased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692eae5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference is also not forecasting\n",
    "* We interpret the confidence interval as what the estimate would be if we collected more $(Y,X)$ data from $F(y|x , \\theta)$\n",
    "* “More data” doesn’t mean data from another context. For example, a confidence interval using data from $F_{t=1}(y|x , \\theta)$ does not directly inform the results we would get from using data from $F_{t=1}(y|x , \\theta)$\n",
    "    * The confidence interval doesn’t directly answer whether $\\hat{\\beta}$ would be the same if we collected data from next month. \n",
    "* If the underlying data generating process changes over time, then we will have model misspecification biases.\n",
    "* Model misspecification cause problems with inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21852b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model misspecification also creates bias\n",
    "* For example, the true model is: $Y = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon $\n",
    "* But we instead estimate this model: $Y = \\beta_1 X_1 + \\beta_2 X_2 + +beta_3 X_2^2 + \\nu $\n",
    "* You have a misspecified model and so your estimate of $\\beta_1$ will be different but can still be statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49983075",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why can’t I just use LASSO and select features?\n",
    "* Since LASSO selects features, we cannot do inference. \n",
    "* LASSO coefficients are estimates using a penalty term for L1 regularization.\n",
    "* Therefore, we cannot say that the coefficients from a LASSO regression are consistent and converge to the true coefficients.\n",
    "* In other words, LASSO coefficients have two interpretations: the causal estimate of $\\hat{\\tau}$ and a bias towards zero to maximize prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e10f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model misspecification in a regression adjustment model\n",
    "* Recall the high-level model algorithm:\n",
    "    1. Estimate the counterfactual control and treatment outcomes $\\hat{Y}_0$ and $\\hat{Y}_1$;\n",
    "    2. Estimate ATE/ATET based on the differences between them.\n",
    "\n",
    "* Ideally, $\\hat{Y}_0$ and $\\hat{Y}_1$ represent the true counterfactual outcomes. But if they are wrong, then the ATE/ATET estimate can still be wrong.\n",
    "* But it can still be statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec93287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we deal with model misspecification?\n",
    "* Each model will generate some model misspecification bias\n",
    "* The recommendation is to try do robustness checks. Try different model specifications, and they should provide similar results\n",
    "    * Transforming features like squares \n",
    "    * Linear and non-linear models\n",
    "* The No Free Lunch Theorem (Wolpert and Macready, 1997) states that there is no model with universally superior performance, so relying on one model is guaranteed to eventually fail you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0562d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review on what an estimate of $\\beta$ is\n",
    "* $\\hat{\\beta} = \\beta + \\text{(Selection Bias)} + \\text{(Model Misspecification Bias)}$\n",
    "* Selection Bias is addressed by assuming we have satisfied the assumptions for a causal interpretation\n",
    "* Model Misspecification Bias is addressed by robustness checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae120c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping\n",
    "* What happens if the estimator is consistent, but we cannot figure out how the estimator is distributed?\n",
    "* Or, if we do not have a large enough sample size for asymptotic properties to kick in.\n",
    "* Let’s numerically calculate how the estimator is distributed.\n",
    "* Recall that the distribution is interpreted as what the estimate would be if we redrew data.\n",
    "* Bootstrapping assumes that the data we have $X$ is sufficient to know what a redrawn dataset looks like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1f87f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap setup\n",
    "* $Y = \\beta X + \\epsilon $\n",
    "* We want to get a bootstrap estimate for the variance of $\\beta$, and we have pairs $(y_1, x_1), (y_2, x_2), ... (y_N, x_N)$.\n",
    "* **Non-parametric bootstrap:**\n",
    "1. Resample $N$ pairs from your sample with replacement $S$ times\n",
    "2. For each bootstrap $s \\in S$, calculate $\\hat{\\beta}_s$\n",
    "3. Use the variance of $\\hat{\\beta}_1, \\hat{\\beta}_2, ... \\hat{\\beta}_S$ for the variance of $\\hat{\\beta}$\n",
    "\n",
    "* **Parametric bootstrap:**\n",
    "1. Calculate the joint distribution of $y | x \\sim F(x, \\theta)$\n",
    "2. Draw $S$ pairs from $F(x,\\theta)$, and do the same as 2. and 3. from the non-parametric bootstrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0a3da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can bootstrap more than just variances\n",
    "* For any given bootstrap $s$, you can calculate all sort of statistics from \n",
    "$Y_s = \\beta_s X_s + \\epsilon_s$\n",
    "    * The p-value, standard error, confidence interval of $\\beta_s$\n",
    "    * Metrics of the regression like: F-statistic, $R^2$, or RMSE\n",
    "* As $S \\rightarrow \\infty$, the variance of bootstrap statistics approaches the truth.\n",
    "\n",
    "* How many we do depends on the question we want to answer. More bootstraps gives us more precision.\n",
    "* As a general practice, $S$ should be large enough that the bootstrapped metric is stable enough.\n",
    "    * Andrews and Buchinsky (2000); Cameron and Trivedi (2005) give us context dependent recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60f98c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final warning about bootstraps\n",
    "* Bootstrapping only works if your estimator is consistent. An estimator is useless for inference if it is not consistent. \n",
    "* For example, you can train an ML model to predict $Y$ based on $X \\in R$ and $W={0,1}$., then use $\\hat{Y}(X,W=1)$ and $\\hat{Y}(X,W=0)$. But unless you can show that $\\hat{Y}(X,W=1) - \\hat{Y}(X,W=0)$ converges to the true treatment effect, then bootstrapping will not let you conduct proper inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834b5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "* We have shown that statistical theorems are necessary to conduct inference for estimates\n",
    "* Statistically significant estimates do not mean you have a causal estimate\n",
    "* Model misspecification biases\n",
    "* Recommendations for understanding model misspecification biases and bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fe136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
