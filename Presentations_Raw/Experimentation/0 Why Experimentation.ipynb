{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8b3468",
   "metadata": {},
   "source": [
    "# Why Experimentation and Role of Noise\n",
    "Julian Hsu\n",
    "29-apr-2023\n",
    "\n",
    "\n",
    "In this notebook we will motivate why AB experimentation (and general causal inference) is useful, and the role of statistial noise. Statistical noise means that we care about Type 1 and Type 2 errors. This will be a key feature of our experimental design.\n",
    "\n",
    "Other topics we can cover are:\n",
    "1. The Stable Unit Treatment Value Assumption (SUTVA) and network effects;\n",
    "2. Sequential testing; \n",
    "3. When randomization fails: causal models; and\n",
    "4. Heterogeneous treatment effects in experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc279072",
   "metadata": {},
   "source": [
    "## Why Experiments?\n",
    "Very often we take actions because we want to understand the return, or lift, from it. For example, we may be interested in launching a new ads campaign, changing the price of a product, or introducing new search query algorithms. How do we know that these actions are actually improvements? That is, they deliver benefit?\n",
    "\n",
    "Once we agree that this decision should be motivated by data, we have several pptions available to us. All these options frame this question as a **causal question of what the impact is from a given action.** These are two options available to us.\n",
    "\n",
    "First is to look at any historical data where we may have be taken similar actions. That is, we are considering increasing a product price and look at when we changed the price in the past. The drawback of historical data is that often these similar actions were not done as part of an experiment. This means that we cannot be sure the changes in outcomes is directly due to the price change, or due to other factors such as other products' prices changing. This means that our estimate using historical, non-experimental data could be **biased.** In the example of the product price, the change in overall revenue is due to the product price change and other products' price change, but we cannot know how much is due to one or the other.\n",
    "\n",
    "The second approach is to do a randomized experiment. In this experiment, we will randomly assign some units to a treatment group and others to a control group. In the case of product price or an advertising campaign, some customers are exposed to the new price or ads campaign, while others are not.\n",
    "\n",
    "Why is the experiment a better approach? Since we have randomly assigned treatment and control, we have isolated the role other factors can play. Other features that can drive the impact are similar between treatment and control. That is, customers that randomly experience different prices or ads campaigns are similar in all ways except for the treatment and control status. This means that when we look at the differences in outcomes between treatment and control, we can safely attribute differencestoo the treatment and control status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1197c3d",
   "metadata": {},
   "source": [
    "### What If? Outcomes\n",
    "We can think about experiments as a superior approach because it gives us the best insight and estimate of units' outcomes under both treatment and control status.\n",
    "\n",
    "Our decision that this is a **causal question** means that we have decided our inherent problem is that we do not know what the outcomes would be under treatment and control are like. That is, we do not know, on average, what the overall revenue would be if we changed the product price, or what the overall revenue is if we used the new campaign or not. These are also known as **counterfactual outcomes.** In our data, we can only directly see outcomes if they are treated, control, but not both.\n",
    "\n",
    "This is another way to think about the **bias** from before. Suppose that in our data, treated customers that saw higher prices for a product also happened to be shopping for more products. In this case, treated customers could be spending more because they are shopping for more products, rather than because they were experiencing higher prices. This means that the outcomes of treated customers is greater than the **counterfactual outcome** of control customers if they faced higher prices.\n",
    "\n",
    "A randomized experiment removes these biases on average because it randomly assigns treatment and control status. If we looked over all the control customers, we should find that they are on average the same as the treatment customers. Then these treatment and control groups are accurate representations of each others **counterfactual outcome.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6c867",
   "metadata": {},
   "source": [
    "## Role of Noise: Hypothesis Testing and Errors\n",
    "\n",
    "Suppose we have done a randomized experiment around price or the new ads campaign. We have verified that the randomization was correctly done, so that we have unbiased estimates of the counterfactual outcomes. We have ruled out bias from different features that drive the outcome, but we have not ruled out bias that comes from natural statistical noise.\n",
    "\n",
    "For example, we know that customer spending is highly unpredictable. There are multiple things that can drive customer spending, and only some of it (income, location) is observable. Experimentation allows us to remove the role that these observed and unobserved factors can have on spending. However, customer spending will still vary among treatment and control units. Experimentation works by making control and treatment units the same *on average.* The natural variation within these can be found of as statistical noise.\n",
    "\n",
    "We will cover in this section how this noise interferes with our ability to test whether the null hypothesis that treatment and control customer spending is the same. Technically, we are trying to gather evidence that either rejects the null hypothesis, or failed to reject the null hypothesis. Here, a Type 1 error would be we reject the null hypothesis, but it was in fact true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9fe70",
   "metadata": {},
   "source": [
    "### Type 1 and Type 2 Errors\n",
    "It's important to recognize that statistical noise is indistinguishable from actual impacts. In one experiment, we see that treatment customers' overall spending is greater than control customers' spending on average. But is this due to the treatment is because treatment customers' just happened to be spending more for other reasons? We can never know this for sure because we do not know the individual ground truth. In other words,  we are missing the **counterfactual outcomes** for each unit. This means that we could conclude that the treatment had an impact, but in actually it did not.\n",
    "\n",
    "There are two types of Errors we could make. We describe them simply first here, and will then connect them to statistical tests.\n",
    "1. **Type 1 Error**(aka False Positive): We incorrectly conclude that the treatment had an impact, but it in fact did not.  \n",
    "2. **Type 2 Error**(aka False Negative): we incorrectly conclude that the treatment had no impact, but it in fact did.\n",
    "\n",
    "Since statistical noise is ever-present in data and experimentation, we will **always run some risk** of making a Type 1 or Type 2 error. Fortunately, separate statistical tests formalize a process for us to assume some Type 1 and Type 2 risk when making a decision.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcdeb7",
   "metadata": {},
   "source": [
    "### Hypothesis Testing and Type 1 Errors\n",
    "\n",
    "Using the variation in outcomes across treatment and control units, statistical tests will can tell us how much Type 1 error risk is associated with a conclusion that we reject or fail to reject the null hypothesis. Summarizing the technical details, a statistical test uses the Law of Large Numbers to estimate what the variation of treatment and control unit differences would be if we had repeated the experiment infinite times.\n",
    "\n",
    "What does it mean to repeat an experiment infinite times?\n",
    "* You may think it would be if we did the experiment again next month, and the month after that, etc. That isn't entirely right because the month may drive differences in outcomes.\n",
    "* You may think if we sampled different customers instead of the ones we had? This is a little closer but still not entirely right. Suppose that we sampled all possible treatment and control customers, we couldn't possible sample other ones. \n",
    "\n",
    "Infinite times means that if we sampled the same customers over and over, but the statistial noise is different. You can think of it as a parallel universe where we sampled the same customers. These are the same customers, but because we did experiments over different parallel universes their outcomes will be different. \n",
    "\n",
    "Now that the hypothesis test takes this estimated parallel universe information to calculate a p-value. This p-value gives us a window into what possible outcomes could have happened in these other parallel universe experiments, we know **how likely it is we see the different in outcomes between treatment and control assuming that our null hypothesis is true.** This bolded point bears repeating. The p-value can tell us how likely it is we see a difference in treatment and control customers driven by noise, assuming that there really is no difference. If the p-value is sufficiently low, then we can reject the null hypothesis. \n",
    "\n",
    "For example, we can find that treatment customers have 5% more spending than control customers, with a p-value of 2%. This means the probability we would have see a 5% difference assuming there were no differences is 2%. But is 2% sufficiently low? Ideally, we would have decided how low a p-value we are willing to accept to reject the null hypothesis. That is, we have already decided a Type 1 error rate (False Positive Rate). The most common you will see is 5%, and less common ones are 1% and 10%. Since 2% is less than our pre-defined 5% threshold, we conclude that treatment customers spend more than control customers by 5% on average. \n",
    "\n",
    "We can take this pre-defined 5% threshold further to define a 95% confidence interval. In this example, we calculated the 95% confidence interval can be from 1% to 8%; the p-value is 2%. This means that if we had this experiment an infinite number of times, 95% of the differences would be between 1% and 8%. \n",
    "\n",
    "You'll notice that our discussion of hypothesis tests, p-values, and confidence intervals hasn't touched on Type 2 errors yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515e4bb",
   "metadata": {},
   "source": [
    "### Type 2 Errors and Statistical Power\n",
    "\n",
    "We use a different type of statistical test to formalize our Type 2 error rate (False Negative Rate). Recall that a Type 2 error is when we conclude treatment and control are not difference, when in fact they are. In other words, we have failed to reject the null hypothesis when we should have. Statistical noise plays a role here because we have incorrectly concluded that difference between treatment and control units was noise and should have been ignored.\n",
    "\n",
    "Why did we think the difference was noise? The difference must have been so small, that we thought it was more likely to be noise than the treatment. The smaller the difference is, the more likely we think it is noise. Suppose that instead of a 5% difference, we found a 0.05% difference. The p-value could have been 20%, meaning that we'd conclude that treatment and control were the same because we adopted a 5% false positive rate.\n",
    "\n",
    "So how large does the difference need to be that the p-value is below our false positive rate of 5%?\n",
    "\n",
    "We also need to incorporate the False Negative Rate. So far we have been considering the probability of seeing the 0.05% difference assuming that the treatment and control are the same. But what about assuming that treatment and control are different, specifically what is the probability of a False Negative Rate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cadfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd631b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb02c24",
   "metadata": {},
   "source": [
    "## Conclusion and Approach\n",
    "\n",
    "1. Define your false positive rate and false negative rates;\n",
    "2. Define your minimum detectable effect (MDE) size;\n",
    "3. Do a power calculation to determine you have enough sample to detect the MDE;\n",
    "4. [optional] Considering whether you can do multiple treatment arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7864d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
